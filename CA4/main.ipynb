{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "obcvQ7_kklVG"
   },
   "source": [
    "### $\\bullet\\;Import\\;Libraries$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lEvM0TP-k4PD",
    "outputId": "7b42dc8b-257a-434b-9af3-b8d24147af52"
   },
   "outputs": [],
   "source": [
    "%pip install torchmetrics\n",
    "%pip install torchview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Grohs0UfklVK"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn             as nn\n",
    "import torch.optim          as optim\n",
    "import torch.nn.functional  as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "import matplotlib.pyplot    as plt\n",
    "import numpy                as np\n",
    "import pandas               as pd\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from torch.utils.data       import TensorDataset, DataLoader, random_split\n",
    "from torchvision            import transforms\n",
    "from torchvision.datasets   import CIFAR10\n",
    "from torchmetrics           import Accuracy\n",
    "from tqdm                   import tqdm\n",
    "from torch.utils.data       import Dataset\n",
    "from PIL                    import Image\n",
    "from math                   import sqrt\n",
    "\n",
    "from torchview import draw_graph\n",
    "from IPython.display import Image, display\n",
    "\n",
    "plt.style.use('grayscale')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LkW-YSEKklVL"
   },
   "source": [
    "### $\\bullet\\;CIFAR\\;Dataset$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "91bmUa5aklVM"
   },
   "source": [
    "#### $Load\\;Dataset$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CIFAR10(root='./cifar10', train=True, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=len(dataset), shuffle=False, num_workers=2)\n",
    "data_iter = iter(loader)\n",
    "images, _ = next(data_iter) \n",
    "\n",
    "mean = images.mean(dim=[0, 2, 3])\n",
    "std = images.std(dim=[0, 2, 3])\n",
    "\n",
    "print(\"Mean per channel :\", mean)\n",
    "print(\"Std per channel  :\", std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qZLzcBv_klVM",
    "outputId": "1c139c57-0954-42dd-cb93-45e9629a732c"
   },
   "outputs": [],
   "source": [
    "transform   = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2470, 0.2435, 0.2616])])\n",
    "\n",
    "train_set   = CIFAR10(root='./cifar10', train=True , download=True, transform=transform)\n",
    "test_set    = CIFAR10(root='./cifar10', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fmDJ-V7LklVN"
   },
   "source": [
    "#### $Train\\;set-Test\\;set\\;Classes$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "efAb3rqaklVN",
    "outputId": "40ca2c54-c574-4577-baef-d1a19cc741e9"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(train_set.class_to_idx.items(), columns=['Class Name', 'Class Index']).style.hide()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "mCRQ4JZDklVN",
    "outputId": "15f73e08-834f-4305-bb7b-1e47be231545"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(train_set.class_to_idx.items(), columns=['Class Name', 'Class Index']).style.hide()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val    = 0\n",
    "        self.avg    = 0\n",
    "        self.sum    = 0\n",
    "        self.count  = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val    = val\n",
    "        self.sum    += val * n\n",
    "        self.count  += n\n",
    "        self.avg    = self.sum / self.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\bullet\\;$ CIFAR10 DataLoader, Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader\n",
    "trainloader = torch.utils.data.DataLoader(train_set, batch_size=128, shuffle=True, num_workers=4)\n",
    "testloader = torch.utils.data.DataLoader(test_set, batch_size=256, shuffle=False, num_workers=4)\n",
    "\n",
    "def denormalize(img, mean, std):\n",
    "    mean = torch.tensor(mean).view(3, 1, 1)\n",
    "    std = torch.tensor(std).view(3, 1, 1)\n",
    "    return img * std + mean\n",
    "\n",
    "# Show one batch of images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "img_grid = torchvision.utils.make_grid(images)\n",
    "img_grid = denormalize(img_grid, mean, std)\n",
    "plt.imshow(img_grid.permute(1, 2, 0).clamp(0, 1))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\bullet\\;$ CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # 7 layers total: 6 conv + final classifier\n",
    "        self.features = nn.Sequential(\n",
    "            # Block 1\n",
    "            nn.Conv2d(3, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            # Block 2\n",
    "            nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            # Block 3\n",
    "            nn.Conv2d(128, 256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU(), nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.classifier = nn.Linear(256 * 4 * 4, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "model = CNN()\n",
    "batch_size = 2\n",
    "size_model = 0\n",
    "for param in model.parameters():\n",
    "    if param.data.is_floating_point():\n",
    "        size_model += param.numel() * torch.finfo(param.data.dtype).bits\n",
    "    else:\n",
    "        size_model += param.numel() * torch.iinfo(param.data.dtype).bits\n",
    "print(f\"model size: {size_model} bit | {size_model / 8e6:.2f} MB\")\n",
    "\n",
    "model_graph = draw_graph(model, input_size=(1, 3, 32, 32), device='meta')\n",
    "display(model_graph.visual_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_layer_outputs(model, input_size):\n",
    "    x = torch.randn(input_size)\n",
    "    print(f\"Input: {x.shape}\")\n",
    "    count = 0\n",
    "    for i, layer in enumerate(model.features):\n",
    "        x = layer(x)\n",
    "        if layer.__class__.__name__ in ['Conv2d']:\n",
    "            count += 1\n",
    "        print(f\"Layer {count} ({layer.__class__.__name__}): {x.shape}\")\n",
    "    x = x.view(x.size(0), -1)\n",
    "    print(f\"Flatten: {x.shape}\")\n",
    "    x = model.classifier(x)\n",
    "    print(f\"Classifier Output: {x.shape}\")\n",
    "\n",
    "model = CNN().to('cpu')\n",
    "print_layer_outputs(model, (1, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "summary(model, input_size=(1, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_model = 0\n",
    "for param in model.parameters():\n",
    "    if param.data.is_floating_point():\n",
    "        size_model += param.numel()\n",
    "    else:\n",
    "        size_model += param.numel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\bullet\\;$ Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loss, optimizer, scheduler\n",
    "\n",
    "using CrossEntropyLoss\n",
    "\n",
    "using Adam with lr=0.001, weight_decay=1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = CNN().to(device)\n",
    "\n",
    "# loss, optimizer, scheduler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop with loss tracking\n",
    "epochs = 15\n",
    "train_accs, test_accs, train_losses, test_losses = [], [], [], []\n",
    "for epoch in range(1, epochs+1):\n",
    "    model.train()\n",
    "    correct, total = 0, 0\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, targets) in enumerate(trainloader, 1):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        _, preds = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += preds.eq(targets).sum().item()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    train_accs.append(100.*correct/total)\n",
    "    train_losses.append(running_loss / total)\n",
    "\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in testloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += preds.eq(targets).sum().item()\n",
    "            loss = criterion(outputs, targets)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "    test_acc = 100. * correct / total\n",
    "    test_losses.append(running_loss / total)\n",
    "    test_accs.append(test_acc)\n",
    "    scheduler.step()\n",
    "    print(f\"Epoch {epoch:2d}: Train Acc = {train_accs[-1]:.2f}%, Test Acc = {test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy and loss curves\n",
    "eps = range(1, epochs+1)\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(eps, train_accs, label='Train Acc')\n",
    "plt.plot(eps, test_accs, label='Test Acc')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.title('Accuracy Curve')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(eps, train_losses, label='Train Loss')\n",
    "plt.plot(eps, test_losses, label='Train Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss Curve')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "baseline_model = copy.deepcopy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in testloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    return 100.*correct/total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1-بررسی هرس یکباره"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ابتدا از روش کاملا تصادفی استفاده می کنیم. در این روش بدون توجه به هیچ چیزی شروع به هرس می کنیم و برایمان تفاوتی بین وزن ها و یال ها وجود ندارد."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pruned_models = []\n",
    "prune_amounts = [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
    "accuracies = []\n",
    "\n",
    "rand_model = dict()\n",
    "\n",
    "for amount in prune_amounts:\n",
    "    model_copy = copy.deepcopy(baseline_model)\n",
    "    for module in model_copy.modules():\n",
    "        if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
    "            prune.random_unstructured(module, name='weight', amount=amount)\n",
    "    pruned_models.append(model_copy)\n",
    "    acc = test(model_copy)\n",
    "    accuracies.append(acc) \n",
    "    rand_model[amount] = model_copy\n",
    "    print(f'Random unstructured prune {amount*100:.0f}%, acc: {acc:.2f}%')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot([amount * 100 for amount in prune_amounts], accuracies, marker='o', linestyle='-', color='b', label='Accuracy')\n",
    "plt.xlabel('Prune Amount (%)')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Accuracy vs Prune Amount in random prune')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "همانطور که انتظار داشتیم این روش اصلا مناسب نیست و از همان ابتدا دقت مدل را نصف می کند"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "حال از l1_unstructured استفاده می کنیم که در این روش وزن هایی که قدر مطلق آنها به صفر نزندیک تر است حذف می شود. در مقابل آن l2 رو داریم که به جای قدر مطلق از توان ۲ \n",
    "استفاده می کند"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_models = []\n",
    "prune_amounts = [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
    "accuracies = []\n",
    "\n",
    "l1_uns_model = dict()\n",
    "\n",
    "for amount in prune_amounts:\n",
    "    model_copy = copy.deepcopy(baseline_model)\n",
    "    for module in model_copy.modules():\n",
    "        if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
    "            prune.l1_unstructured(module, name='weight', amount=amount)\n",
    "    pruned_models.append(model_copy)\n",
    "    acc = test(model_copy)\n",
    "    accuracies.append(acc) \n",
    "    l1_uns_model[amount] = model_copy\n",
    "    print(f'l1 unstructured prune {amount*100:.0f}%, acc: {acc:.2f}%')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot([amount * 100 for amount in prune_amounts], accuracies, marker='o', linestyle='-', color='b', label='Accuracy')\n",
    "plt.xlabel('Prune Amount (%)')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Accuracy vs Prune Amount in l1 prune')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "همانطور که در نمودار مشخص است هرس تا ۲۰ درصد تقریبا بی تاثیر است ولی باقی موارد بیش از ۱ درصد تاثیر دارند که مناسب نیست و همچنین افت دقت شدیدی بعد از حذف ۶۰ درصد شامل هستیم که دقت نصف شده"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "باید توجه کرد که در این روش ما با پیمایش روی شبکه در هر لایه به صورت جداگانه حرص انجام می دهیم. می شود با تجمیع وزن ها نیز این کار را انجام داد. از آنجایی که روش اول تصادفی است در نتیجه آن تفاوتی ایجاد نمی شود اما ممکن است در روش دوم تفاوت ایجاد کند که در ادامه بررسی می کنیم.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_models = []\n",
    "prune_amounts = [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
    "accuracies = []\n",
    "\n",
    "l1_uns_glo_model = dict()\n",
    "\n",
    "parameters_to_prune = []\n",
    "for module in baseline_model.modules():\n",
    "    if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
    "        parameters_to_prune.append((module, 'weight'))\n",
    "\n",
    "for amount in prune_amounts:\n",
    "    model_copy = copy.deepcopy(baseline_model)\n",
    "    parameters = []\n",
    "    for module in model_copy.modules():\n",
    "        if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
    "            parameters.append((module, 'weight'))\n",
    "\n",
    "    prune.global_unstructured(\n",
    "        parameters,\n",
    "        pruning_method=prune.L1Unstructured,\n",
    "        amount=amount,\n",
    "    )\n",
    "    \n",
    "    pruned_models.append(model_copy)\n",
    "    acc = test(model_copy)\n",
    "    accuracies.append(acc) \n",
    "    l1_uns_glo_model[amount] = model_copy\n",
    "    print(f'l1-global unstructured prune {amount*100:.0f}%, acc: {acc:.2f}%')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot([amount * 100 for amount in prune_amounts], accuracies, marker='o', linestyle='-', color='b', label='Accuracy')\n",
    "plt.xlabel('Prune Amount (%)')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Accuracy vs Prune Amount in l1-global prune')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "همانطور که در نمودار مشخص است با این کار تقریبا تا ۴۰ درصد حرص قابل قبول است پس با این کار تحمل مدل در حرص ۲ برابر شد"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2- هرس مرحله به مرحله"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs):\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        correct, total = 0, 0\n",
    "        running_loss = 0.0\n",
    "        for i, (inputs, targets) in enumerate(trainloader, 1):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            _, preds = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += preds.eq(targets).sum().item()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        train_accs.append(100. * correct / total)\n",
    "        train_losses.append(running_loss / total)\n",
    "\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        running_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in testloader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, preds = outputs.max(1)\n",
    "                total += targets.size(0)\n",
    "                correct += preds.eq(targets).sum().item()\n",
    "                loss = criterion(outputs, targets)\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        test_acc = 100. * correct / total\n",
    "        test_accs.append(test_acc)\n",
    "        test_losses.append(running_loss / total)\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"Epoch {epoch}/{epochs} - Train Acc: {train_accs[-1]:.2f}%, Test Acc: {test_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_prune = 0.2\n",
    "end_prune = 0.8\n",
    "step_count = int((end_prune - start_prune) / 0.1) + 1\n",
    "fine_tune_epochs = 4\n",
    "\n",
    "model = copy.deepcopy(baseline_model)\n",
    "parameters_to_prune = [\n",
    "    (m, 'weight') for m in model.modules()\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear)\n",
    "]\n",
    "\n",
    "accuracies = []\n",
    "prune_levels = []\n",
    "\n",
    "l1_uns_glob_step = dict()\n",
    "\n",
    "current_pruned = 0.0\n",
    "target_pruned_list = [round(p, 2) for p in list(torch.arange(start_prune, end_prune + 0.001, 0.1).tolist())]\n",
    "\n",
    "for target in target_pruned_list:\n",
    "    remaining = 1.0 - current_pruned\n",
    "    step_amount = (target - current_pruned) / remaining  \n",
    "\n",
    "    prune.global_unstructured(\n",
    "        parameters_to_prune,\n",
    "        pruning_method=prune.L1Unstructured,\n",
    "        amount=step_amount\n",
    "    )\n",
    "    current_pruned = target\n",
    "\n",
    "    # loss, optimizer, scheduler\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=fine_tune_epochs)\n",
    "\n",
    "    train(fine_tune_epochs)\n",
    "\n",
    "    acc = test(model)\n",
    "    accuracies.append(acc)\n",
    "    prune_levels.append(target * 100)\n",
    "    l1_uns_glob_step[target] = copy.deepcopy(model)\n",
    "    print(f'Pruned to {target*100:.0f}%, Accuracy: {acc:.2f}%')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(prune_levels, accuracies, marker='o', linestyle='-', color='b', label='Accuracy')\n",
    "plt.xlabel('Prune Amount (%)')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Accuracy vs Prune Amount in L1-Global Iterative Prune')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "با این کار دقت مدل تا ۵۰ درصد خوب است و ختی از ۴۰ درصد حالت قبلی بهتر است و همچنین دقت تمامی حالات بالا تر است مثلا در هرس ۸۰ درصد دقت ۶۳ درصد می شود در مقابل ۲۴ درصد"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3- هرس ساختارمند"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_prune = 0.2\n",
    "end_prune = 0.8\n",
    "step_count = int((end_prune - start_prune) / 0.1) + 1\n",
    "fine_tune_epochs = 4\n",
    "\n",
    "model = copy.deepcopy(baseline_model)\n",
    "\n",
    "accuracies = []\n",
    "prune_levels = []\n",
    "\n",
    "l2_struc_step_model = dict()\n",
    "\n",
    "current_pruned = 0.0\n",
    "target_pruned_list = [round(p, 2) for p in list(torch.arange(start_prune, end_prune + 0.001, 0.1).tolist())]\n",
    "\n",
    "for target in target_pruned_list:\n",
    "    remaining = 1.0 - current_pruned\n",
    "    step_amount = (target - current_pruned) / remaining \n",
    "\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            prune.ln_structured(module, name='weight', amount=step_amount, n=2, dim=0) \n",
    "\n",
    "    current_pruned = target\n",
    "\n",
    "    # loss, optimizer, scheduler\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=fine_tune_epochs)\n",
    "\n",
    "    train(fine_tune_epochs)\n",
    "\n",
    "    acc = test(model)\n",
    "    accuracies.append(acc)\n",
    "    prune_levels.append(target * 100)\n",
    "    l2_struc_step_model[target] = copy.deepcopy(model)\n",
    "    print(f'L2 Structured Pruned to {target*100:.0f}%, Accuracy: {acc:.2f}%')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(prune_levels, accuracies, marker='o', linestyle='-', color='g', label='L2 Structured Accuracy')\n",
    "plt.xlabel('Prune Amount (%)')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Accuracy vs Prune Amount in L2-Structured Iterative Prune')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_prune = 0.2\n",
    "end_prune = 0.8\n",
    "step_count = int((end_prune - start_prune) / 0.1) + 1\n",
    "fine_tune_epochs = 4\n",
    "\n",
    "model = copy.deepcopy(baseline_model)\n",
    "\n",
    "accuracies = []\n",
    "prune_levels = []\n",
    "\n",
    "l1_struc_step_model = dict()\n",
    "\n",
    "current_pruned = 0.0\n",
    "target_pruned_list = [round(p, 2) for p in list(torch.arange(start_prune, end_prune + 0.001, 0.1).tolist())]\n",
    "\n",
    "for target in target_pruned_list:\n",
    "    remaining = 1.0 - current_pruned\n",
    "    step_amount = (target - current_pruned) / remaining \n",
    "\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            prune.ln_structured(module, name='weight', amount=step_amount, n=1, dim=0) \n",
    "\n",
    "    current_pruned = target\n",
    "\n",
    "    # loss, optimizer, scheduler\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=fine_tune_epochs)\n",
    "\n",
    "    train(fine_tune_epochs)\n",
    "\n",
    "    acc = test(model)\n",
    "    accuracies.append(acc)\n",
    "    prune_levels.append(target * 100)\n",
    "    l1_struc_step_model[target] = copy.deepcopy(model)\n",
    "    print(f'L2 Structured Pruned to {target*100:.0f}%, Accuracy: {acc:.2f}%')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(prune_levels, accuracies, marker='o', linestyle='-', color='g', label='L1 Structured Accuracy')\n",
    "plt.xlabel('Prune Amount (%)')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Accuracy vs Prune Amount in L2-Structured Iterative Prune')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding file in block (for using in kaggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Function\n",
    "\n",
    "class Quantize(torch.nn.Module):\n",
    "    def __init__(self, n_bits, n_frac, sign=True):\n",
    "        super(Quantize, self).__init__()\n",
    "        self.n_bits = n_bits\n",
    "        self.n_frac = n_frac\n",
    "        i = self.n_bits - self.n_frac\n",
    "        if sign:\n",
    "            self.max = float(2 ** (i - 1) - 2 ** (-self.n_frac))\n",
    "            self.min = float(-2 ** (i - 1))\n",
    "        else:\n",
    "            self.max = float(2 ** (i) - 2 ** (-self.n_frac))\n",
    "            self.min = 0.0\n",
    "\n",
    "    def forward(self, x):\n",
    "        if torch.onnx.is_in_onnx_export():\n",
    "            return x\n",
    "        else:\n",
    "            n = float(2 ** self.n_frac)\n",
    "            xx = torch.floor(x * n) / n\n",
    "            clipped = torch.clip(xx, self.min, self.max)\n",
    "            return clipped\n",
    "\n",
    "class Round(Function):\n",
    "    @staticmethod\n",
    "    def forward(self, input):\n",
    "        sign = torch.sign(input)\n",
    "        output = sign * torch.floor(torch.abs(input) + 0.5)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(self, grad_output):\n",
    "        grad_input = grad_output.clone()\n",
    "        return grad_input\n",
    "\n",
    "class ALSQPlus(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, weight, alpha, beta, g, Qn, Qp):\n",
    "        ctx.save_for_backward(weight, alpha, beta)\n",
    "        ctx.other = g, Qn, Qp\n",
    "        w_q = Round.apply(torch.div((weight - beta), alpha).clamp(Qn, Qp))\n",
    "        w_q = w_q * alpha + beta\n",
    "        return w_q\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_weight):\n",
    "        weight, alpha, beta = ctx.saved_tensors\n",
    "        g, Qn, Qp = ctx.other\n",
    "        q_w = (weight - beta) / alpha\n",
    "        smaller = (q_w < Qn).float()\n",
    "        bigger = (q_w > Qp).float()\n",
    "        between = 1.0 - smaller -bigger\n",
    "        grad_alpha = ((smaller * Qn + bigger * Qp +\n",
    "            between * Round.apply(q_w) - between * q_w)*grad_weight * g).sum().unsqueeze(dim=0)\n",
    "        grad_beta = ((smaller + bigger) * grad_weight * g).sum().unsqueeze(dim=0)\n",
    "        grad_weight = between * grad_weight\n",
    "        return grad_weight, grad_alpha, grad_beta, None, None, None\n",
    "\n",
    "\n",
    "class WLSQPlus(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, weight, alpha, g, Qn, Qp, per_channel):\n",
    "        ctx.save_for_backward(weight, alpha)\n",
    "        ctx.other = g, Qn, Qp, per_channel\n",
    "        if per_channel:\n",
    "            sizes = weight.size()\n",
    "            # weight = weight.contiguous().view(weight.size()[0], -1)\n",
    "            weight = weight.view(weight.size()[0], -1)\n",
    "            weight = torch.transpose(weight, 0, 1)\n",
    "            alpha = torch.broadcast_to(alpha, weight.size())\n",
    "            wq = Round.apply(torch.div(weight, alpha).clamp(Qn, Qp))\n",
    "            w_q = wq * alpha\n",
    "            w_q = torch.transpose(w_q, 0, 1)\n",
    "            # w_q = w_q.contiguous().view(sizes)\n",
    "            w_q = w_q.view(sizes)\n",
    "            wq = torch.transpose(wq, 0, 1).view(sizes)\n",
    "        else:\n",
    "            wq = Round.apply(torch.div(weight, alpha).clamp(Qn, Qp))\n",
    "            w_q = wq * alpha\n",
    "        return w_q, wq.detach()\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_weight, gwq):\n",
    "        weight, alpha = ctx.saved_tensors\n",
    "        g, Qn, Qp, per_channel = ctx.other\n",
    "        if per_channel:\n",
    "            sizes = weight.size()\n",
    "            # weight = weight.contiguous().view(weight.size()[0], -1)\n",
    "            weight = weight.view(weight.size()[0], -1)\n",
    "            weight = torch.transpose(weight, 0, 1)\n",
    "            alpha = torch.broadcast_to(alpha, weight.size())\n",
    "            q_w = weight / alpha\n",
    "            q_w = torch.transpose(q_w, 0, 1)\n",
    "            # q_w = q_w.contiguous().view(sizes)\n",
    "            q_w = q_w.view(sizes)\n",
    "        else:\n",
    "            q_w = weight / alpha\n",
    "        smaller = (q_w < Qn).float()\n",
    "        bigger = (q_w > Qp).float()\n",
    "        between = 1.0 - smaller -bigger\n",
    "        if per_channel:\n",
    "            grad_alpha = ((smaller * Qn + bigger * Qp +\n",
    "                between * Round.apply(q_w) - between * q_w)*grad_weight * g)\n",
    "            # grad_alpha = grad_alpha.contiguous().view(grad_alpha.size()[0], -1).sum(dim=1)\n",
    "            grad_alpha = grad_alpha.view(grad_alpha.size()[0], -1).sum(dim=1)\n",
    "        else:\n",
    "            grad_alpha = ((smaller * Qn + bigger * Qp +\n",
    "                between * Round.apply(q_w) - between * q_w)*grad_weight * g).sum().unsqueeze(dim=0)\n",
    "        grad_weight = between * grad_weight\n",
    "        return grad_weight, grad_alpha, None, None, None, None\n",
    "\n",
    "class LSQPlusActivationQuantizer(nn.Module):\n",
    "    def __init__(self, a_bits, all_positive=False,batch_init = 20):\n",
    "        super(LSQPlusActivationQuantizer, self).__init__()\n",
    "        self.a_bits = nn.Parameter(torch.tensor(a_bits), requires_grad=False)\n",
    "        self.s_bits = 20\n",
    "        self.n = 1 << 16\n",
    "        self.all_positive = all_positive\n",
    "        self.batch_init = batch_init\n",
    "        if self.all_positive:\n",
    "            # unsigned activation is quantized to [0, 2^b-1]\n",
    "            self.Qn = 0\n",
    "            self.Qp = 2 ** a_bits - 1\n",
    "        else:\n",
    "            # signed weight/activation is quantized to [-2^(b-1), 2^(b-1)-1]\n",
    "            self.Qn = - 2 ** (a_bits - 1)\n",
    "            self.Qp = 2 ** (a_bits - 1) - 1\n",
    "        self.s = torch.nn.Parameter(torch.ones(1).squeeze(0), requires_grad=True)\n",
    "        self.beta = torch.nn.Parameter(torch.ones(1).squeeze(0), requires_grad=True)\n",
    "        self.g = torch.nn.Parameter(torch.ones(1).squeeze(0), requires_grad=True)\n",
    "        self.register_buffer('init_state', torch.zeros(1))\n",
    "\n",
    "    def forward(self, activation):\n",
    "        if self.training:\n",
    "            if self.init_state==0:\n",
    "                self.g.data = torch.tensor(1.0/math.sqrt(activation.numel() * self.Qp))\n",
    "                mina = torch.min(activation.detach())\n",
    "                self.s.data = (torch.max(activation.detach()) - mina)/(self.Qp-self.Qn)\n",
    "                self.beta.data = mina - self.s.data *self.Qn\n",
    "                self.init_state += 1\n",
    "            elif self.init_state<self.batch_init:\n",
    "                mina = torch.min(activation.detach())\n",
    "                self.s.data = self.s.data*0.9 + 0.1*(torch.max(activation.detach()) - mina)/(self.Qp-self.Qn)\n",
    "                self.beta.data = self.s.data*0.9 + 0.1* (mina - self.s.data * self.Qn)\n",
    "                self.init_state += 1\n",
    "            elif self.init_state==self.batch_init:\n",
    "                self.init_state += 1\n",
    "\n",
    "        if self.a_bits.item() == 32:\n",
    "            q_a = activation\n",
    "        elif self.a_bits.item() == 1:\n",
    "            print('！Binary quantization is not supported ！')\n",
    "            assert self.a_bits.item() != 1\n",
    "        else:\n",
    "            q_a = ALSQPlus.apply(activation, self.s, self.beta, self.g, self.Qn, self.Qp)\n",
    "        return q_a\n",
    "\n",
    "class LSQPlusWeightQuantizer(nn.Module):\n",
    "    def __init__(self, w_bits, all_positive=False, per_channel=False, batch_init = 20, shape=(1,), saved=True):\n",
    "        super(LSQPlusWeightQuantizer, self).__init__()\n",
    "        self.w_bits = nn.Parameter(torch.tensor(w_bits), requires_grad=False)\n",
    "        self.s_bits = 20\n",
    "        self.n = 1 << 16\n",
    "        self.all_positive = all_positive\n",
    "        self.batch_init = batch_init\n",
    "        if self.all_positive:\n",
    "            # unsigned activation is quantized to [0, 2^b-1]\n",
    "            self.Qn = 0\n",
    "            self.Qp = 2 ** w_bits - 1\n",
    "        else:\n",
    "            # signed weight/activation is quantized to [-2^(b-1), 2^(b-1)-1]\n",
    "            self.Qn = - 2 ** (w_bits - 1)\n",
    "            self.Qp = 2 ** (w_bits - 1) - 1\n",
    "        self.per_channel = per_channel\n",
    "        self.register_buffer('init_state', torch.zeros(1))\n",
    "        if not per_channel:\n",
    "           self.s = torch.nn.Parameter(torch.ones(1).squeeze(0), requires_grad=True)\n",
    "        else:\n",
    "            self.s = torch.nn.Parameter(torch.ones(shape[0]), requires_grad=True)\n",
    "\n",
    "        self.saved = saved\n",
    "        if saved:\n",
    "            self.wq = torch.nn.Parameter(torch.ones(shape), requires_grad=True)\n",
    "        self.g = torch.nn.Parameter(torch.ones(1).squeeze(0), requires_grad=True)\n",
    "        \n",
    "    def forward(self, weight):\n",
    "        '''\n",
    "        For this work, each layer of weights and each layer of activations has a distinct step size, represented\n",
    "as an fp32 value, initialized to 2h|v|i/√OP , computed on either the initial weights values or the first\n",
    "batch of activations, respectively\n",
    "        '''\n",
    "        if self.training:\n",
    "            if self.init_state==0:\n",
    "                self.div = 2**self.w_bits.item()-1\n",
    "                self.g.data = torch.tensor(1.0/math.sqrt(weight.numel() * self.Qp))\n",
    "                if self.per_channel:\n",
    "                    # weight_tmp = weight.detach().contiguous().view(weight.size()[0], -1)\n",
    "                    weight_tmp = weight.detach().view(weight.size()[0], -1)\n",
    "                    mean = torch.mean(weight_tmp, dim=1)\n",
    "                    std = torch.std(weight_tmp, dim=1)\n",
    "                    v, _ = torch.max(torch.stack([torch.abs(mean-3*std), torch.abs(mean + 3*std)]), dim=0)\n",
    "                    self.s.data = v/self.div\n",
    "                else:\n",
    "                    mean = torch.mean(weight.detach())\n",
    "                    std = torch.std(weight.detach())\n",
    "                    self.s.data = max([torch.abs(mean-3*std), torch.abs(mean + 3*std)])/self.div\n",
    "                self.init_state += 1\n",
    "            elif self.init_state<self.batch_init:\n",
    "                self.div = 2**self.w_bits.item()-1\n",
    "                if self.per_channel:\n",
    "                    # weight_tmp = weight.detach().contiguous().view(weight.size()[0], -1)\n",
    "                    weight_tmp = weight.detach().view(weight.size()[0], -1)\n",
    "                    mean = torch.mean(weight_tmp, dim=1)\n",
    "                    std = torch.std(weight_tmp, dim=1)\n",
    "                    v, _ = torch.max(torch.stack([torch.abs(mean-3*std), torch.abs(mean + 3*std)]), dim=0)\n",
    "                    self.s.data = v*0.9 + 0.1*v/self.div\n",
    "                else:\n",
    "                    mean = torch.mean(weight.detach())\n",
    "                    std = torch.std(weight.detach())\n",
    "                    self.s.data = self.s.data*0.9 + 0.1*max([torch.abs(mean-3*std), torch.abs(mean + 3*std)])/self.div\n",
    "                self.init_state += 1\n",
    "            elif self.init_state==self.batch_init:\n",
    "                self.init_state += 1\n",
    "\n",
    "        if self.w_bits.item() == 32:\n",
    "            w_q = weight\n",
    "        elif self.w_bits.item() == 1:\n",
    "            print('！Binary quantization is not supported ！')\n",
    "            assert self.w_bits.item() != 1\n",
    "        else:\n",
    "            if self.saved:\n",
    "                w_q, self.wq.data = WLSQPlus.apply(weight, self.s, self.g, self.Qn, self.Qp, self.per_channel)\n",
    "            else:\n",
    "                w_q, _ = WLSQPlus.apply(weight, self.s, self.g, self.Qn, self.Qp, self.per_channel)\n",
    "\n",
    "        return w_q\n",
    "\n",
    "class QuantConv2d(nn.Conv2d):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, \n",
    "                 w_bits=8, a_bits=8, all_positive=False, per_channel=False, batch_init=20):\n",
    "        super(QuantConv2d, self).__init__(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias)\n",
    "        \n",
    "        # Weight quantizer\n",
    "        shape = self.weight.shape\n",
    "        self.weight_quantizer = LSQPlusWeightQuantizer(w_bits, all_positive, per_channel, batch_init, shape)\n",
    "        \n",
    "        # Activation quantizer\n",
    "        self.act_quantizer = LSQPlusActivationQuantizer(a_bits, all_positive, batch_init)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Quantize input activations\n",
    "        x_q = self.act_quantizer(x)\n",
    "        \n",
    "        # Quantize weights\n",
    "        w_q = self.weight_quantizer(self.weight)\n",
    "        \n",
    "        # Use quantized values for convolution\n",
    "        output = F.conv2d(\n",
    "            x_q, w_q, self.bias, self.stride, self.padding, self.dilation, self.groups\n",
    "        )\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "class QuantLinear(nn.Linear):\n",
    "    def __init__(self, in_features, out_features, bias=True, \n",
    "                 w_bits=8, a_bits=8, all_positive=False, per_channel=False, batch_init=20):\n",
    "        super(QuantLinear, self).__init__(in_features, out_features, bias)\n",
    "        \n",
    "        # Weight quantizer\n",
    "        shape = self.weight.shape\n",
    "        self.weight_quantizer = LSQPlusWeightQuantizer(w_bits, all_positive, per_channel, batch_init, shape)\n",
    "        \n",
    "        # Activation quantizer\n",
    "        self.act_quantizer = LSQPlusActivationQuantizer(a_bits, all_positive, batch_init)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Quantize input activations\n",
    "        x_q = self.act_quantizer(x)\n",
    "        \n",
    "        # Quantize weights\n",
    "        w_q = self.weight_quantizer(self.weight)\n",
    "        \n",
    "        # Use quantized values for linear operation\n",
    "        output = F.linear(x_q, w_q, self.bias)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "def prepare(model, w_bits=8, a_bits=8, all_positive=False, per_channel=False, batch_init=20):\n",
    "    \"\"\"\n",
    "    Replaces Conv2d and Linear layers in the model with QuantConv2d and QuantLinear layers.\n",
    "    \n",
    "    Args:\n",
    "        model: The model to be quantized\n",
    "        w_bits: Number of bits for weight quantization\n",
    "        a_bits: Number of bits for activation quantization\n",
    "        all_positive: Whether the quantization is all positive\n",
    "        per_channel: Whether to use per-channel quantization for weights\n",
    "        batch_init: Number of batches for quantizer initialization\n",
    "        \n",
    "    Returns:\n",
    "        Quantized model with replaced layers\n",
    "    \"\"\"\n",
    "    quant_model = copy.deepcopy(model)\n",
    "    \n",
    "    for name, module in list(quant_model.named_children()):\n",
    "        # If it's a container (like Sequential), recursively apply to its children\n",
    "        if len(list(module.children())) > 0:\n",
    "            setattr(quant_model, name, prepare(module, w_bits, a_bits, all_positive, per_channel, batch_init))\n",
    "        \n",
    "        # If it's a Conv2d layer, replace it with QuantConv2d\n",
    "        elif isinstance(module, nn.Conv2d):\n",
    "            quant_conv = QuantConv2d(\n",
    "                module.in_channels, \n",
    "                module.out_channels,\n",
    "                module.kernel_size,\n",
    "                module.stride,\n",
    "                module.padding,\n",
    "                module.dilation,\n",
    "                module.groups,\n",
    "                module.bias is not None,\n",
    "                w_bits,\n",
    "                a_bits,\n",
    "                all_positive,\n",
    "                per_channel,\n",
    "                batch_init\n",
    "            )\n",
    "            \n",
    "            # Copy weights and bias if exists\n",
    "            quant_conv.weight.data = module.weight.data.clone()\n",
    "            if module.bias is not None:\n",
    "                quant_conv.bias.data = module.bias.data.clone()\n",
    "                \n",
    "            setattr(quant_model, name, quant_conv)\n",
    "            \n",
    "        # If it's a Linear layer, replace it with QuantLinear\n",
    "        elif isinstance(module, nn.Linear):\n",
    "            quant_linear = QuantLinear(\n",
    "                module.in_features,\n",
    "                module.out_features,\n",
    "                module.bias is not None,\n",
    "                w_bits,\n",
    "                a_bits,\n",
    "                all_positive,\n",
    "                per_channel,\n",
    "                batch_init\n",
    "            )\n",
    "            \n",
    "            # Copy weights and bias if exists\n",
    "            quant_linear.weight.data = module.weight.data.clone()\n",
    "            if module.bias is not None:\n",
    "                quant_linear.bias.data = module.bias.data.clone()\n",
    "                \n",
    "            setattr(quant_model, name, quant_linear)\n",
    "    \n",
    "    return quant_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3- Quantize base module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_acc_loss():\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(1, epochs+1), train_accs, label='Train Acc')\n",
    "    plt.plot(range(1, epochs+1), test_accs, label='Test Acc')\n",
    "    plt.xlabel('Epoch'); plt.ylabel('Accuracy (%)'); plt.legend(); plt.title('Accuracy')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(1, epochs+1), train_losses, label='Train Loss')\n",
    "    plt.plot(range(1, epochs+1), test_losses, label='Test Loss')\n",
    "    plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.legend(); plt.title('Loss')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = copy.deepcopy(baseline_model)\n",
    "model = prepare(model, w_bits=8, a_bits=8).to(device)\n",
    "\n",
    "epochs = 10\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accs, test_accs, train_losses, test_losses = [], [], [], []\n",
    "\n",
    "train(epochs)\n",
    "\n",
    "acc = test(model)\n",
    "print(f'quantize Accuracy: {acc:.2f}%')\n",
    "q8_model = model\n",
    "\n",
    "plot_acc_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"model size: {size_model} params | {size_model * 8 / 8e6:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "دقت مدل به اندازه مدل اولیه است و کاهش دقت نداشتیم"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For 6, 4 bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = copy.deepcopy(baseline_model)\n",
    "model = prepare(model, w_bits=6, a_bits=6).to(device)\n",
    "\n",
    "epochs = 10\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accs, test_accs, train_losses, test_losses = [], [], [], []\n",
    "\n",
    "train(epochs)\n",
    "\n",
    "acc = test(model)\n",
    "print(f'quantize Accuracy: {acc:.2f}%')\n",
    "q6_model = model\n",
    "\n",
    "plot_acc_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"model size: {size_model} params | {size_model * 6 / 8e6:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = copy.deepcopy(baseline_model)\n",
    "model = prepare(model, w_bits=4, a_bits=4).to(device)\n",
    "\n",
    "epochs = 10\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accs, test_accs, train_losses, test_losses = [], [], [], []\n",
    "\n",
    "train(epochs)\n",
    "\n",
    "acc = test(model)\n",
    "print(f'quantize Accuracy: {acc:.2f}%')\n",
    "q4_model = model\n",
    "\n",
    "plot_acc_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"model size: {size_model} params | {size_model * 4 / 8e6:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "در ۴ بیت ۰.۶ درصد افت دقت و در ۶ بیت ۰.۲۵ درصد افت دقت داشتیم"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best prune model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l1_uns_glob_step : 0.8, 0.6, 0.4\n",
    "\n",
    "for l1 in [0.8, 0.6, 0.4]:\n",
    "    for q_bit in [8, 6]:\n",
    "        s_model = size_model * (1-l1)\n",
    "        print(f\"L1-Global Iterative Prune with {l1*100:.0f}% and quantize with {q_bit}bits size: {s_model:.0f} params | {s_model * q_bit / 8e6:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 8\n",
    "# first for l1 % and second for q level\n",
    "q_l1_uns_glob_step = dict()\n",
    "for l1 in [0.8, 0.6, 0.4]:\n",
    "    q_l1_uns_glob_step[l1] = dict()\n",
    "    for q_bit in [8, 6]:\n",
    "        model = copy.deepcopy(l1_uns_glob_step[l1])\n",
    "        model = prepare(model, w_bits=q_bit, a_bits=q_bit).to(device)\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "        train_accs, test_accs, train_losses, test_losses = [], [], [], []\n",
    "        \n",
    "        train(epochs)\n",
    "        \n",
    "        acc = test(model)\n",
    "        print(f'L1-Global Iterative Prune with {l1*100:.0f}% and quantize with {q_bit}bits Accuracy: {acc:.2f}%')\n",
    "        q_l1_uns_glob_step[l1][q_bit] = copy.deepcopy(model)\n",
    "        \n",
    "        plot_acc_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "در ابتدا دیدیم مدل\n",
    "\n",
    "L1-Global Iterative\n",
    "\n",
    "از همه بهتر عمل کرده بود\n",
    "با مقدادیر مختلف آن مدل را کوانتایز کردیم و آموزش دادیم و با توجه به مقادیر به نظر بیشترین فشرده سازی یعنی ۸۰ درصد هرس و ۶ بیت دقت ۸۶.۹ را می دهد که تنها ۰.۷ کاهش دقت دارد ولی حجم آن به ۰.۳۶ مگابایت می رسد."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "obcvQ7_kklVG",
    "91bmUa5aklVM",
    "fmDJ-V7LklVN",
    "8Q9W8D0OklVO",
    "rADT50VGklVP",
    "EywUImLHklVP",
    "f6NsPgClklVQ",
    "VlnWdl9tklVT",
    "vI91CqGqklVU",
    "qav4WUCpklVU",
    "SGGI-jhlklVV",
    "3gXctq1kklVW",
    "VrCPhqxFklVX",
    "G8Sc5QxYklVY"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
